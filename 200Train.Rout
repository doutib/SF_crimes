
R version 3.2.2 (2015-08-14) -- "Fire Safety"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> print('load data')
[1] "load data"
> source("Load_data.R")
> # Training data set
> df = crimes
> print('load features')
[1] "load features"
> source("Features.R")
> 
> # Include Features ---------------------------------------------------------
> 
> data = as.data.frame(cbind(crimes$Category,features)) # CATEGORY
> 
> 
> # Model Training -----------------------------------------------------------
> 
> require(xgboost)
Loading required package: xgboost
> y=as.numeric(data[,1])-1 # CATEGORY 
> 
> trainMatrix=as.matrix(data[,-1])
> numberOfClasses=max(y)+1
> param <- list("objective" = "multi:softprob",
+               "eval_metric" = "mlogloss",
+               "num_class" = numberOfClasses)
> cv.nround = 200
> cv.nfold = 3
> 
> print('boosting with cv')
[1] "boosting with cv"
> bst.cv = xgb.cv(param=param, data = trainMatrix, label = y, 
+                 nfold = cv.nfold, nrounds = cv.nround)
[0]	train-mlogloss:3.131484+0.002547	test-mlogloss:3.135426+0.003619
[1]	train-mlogloss:2.945235+0.000145	test-mlogloss:2.951257+0.001682
[2]	train-mlogloss:2.825576+0.000966	test-mlogloss:2.833698+0.002453
[3]	train-mlogloss:2.741026+0.000662	test-mlogloss:2.750912+0.002179
[4]	train-mlogloss:2.677405+0.000747	test-mlogloss:2.688952+0.002348
[5]	train-mlogloss:2.626914+0.000711	test-mlogloss:2.640150+0.001072
[6]	train-mlogloss:2.586412+0.000742	test-mlogloss:2.601261+0.001220
[7]	train-mlogloss:2.553815+0.000985	test-mlogloss:2.570378+0.001472
[8]	train-mlogloss:2.527336+0.000563	test-mlogloss:2.545573+0.001654
[9]	train-mlogloss:2.505219+0.000151	test-mlogloss:2.525165+0.002033
[10]	train-mlogloss:2.486227+0.000417	test-mlogloss:2.507832+0.001969
[11]	train-mlogloss:2.470292+0.000647	test-mlogloss:2.493571+0.002772
[12]	train-mlogloss:2.456383+0.000715	test-mlogloss:2.481348+0.003059
[13]	train-mlogloss:2.444244+0.000784	test-mlogloss:2.470846+0.003121
[14]	train-mlogloss:2.434335+0.000844	test-mlogloss:2.462609+0.003098
[15]	train-mlogloss:2.425323+0.001282	test-mlogloss:2.455180+0.003173
[16]	train-mlogloss:2.417017+0.000725	test-mlogloss:2.448536+0.002772
[17]	train-mlogloss:2.409613+0.000727	test-mlogloss:2.442766+0.002607
[18]	train-mlogloss:2.402669+0.000341	test-mlogloss:2.437481+0.002407
[19]	train-mlogloss:2.396893+0.000364	test-mlogloss:2.433391+0.002277
[20]	train-mlogloss:2.391265+0.000415	test-mlogloss:2.429406+0.002133
[21]	train-mlogloss:2.386064+0.000567	test-mlogloss:2.425858+0.002009
[22]	train-mlogloss:2.380953+0.000358	test-mlogloss:2.422370+0.002234
[23]	train-mlogloss:2.376370+0.000646	test-mlogloss:2.419434+0.001856
[24]	train-mlogloss:2.372103+0.001088	test-mlogloss:2.416739+0.001663
[25]	train-mlogloss:2.368237+0.001074	test-mlogloss:2.414472+0.001652
[26]	train-mlogloss:2.364537+0.001186	test-mlogloss:2.412410+0.001362
[27]	train-mlogloss:2.360872+0.001193	test-mlogloss:2.410298+0.001364
[28]	train-mlogloss:2.357369+0.001132	test-mlogloss:2.408334+0.001581
[29]	train-mlogloss:2.354087+0.001333	test-mlogloss:2.406566+0.001383
[30]	train-mlogloss:2.351120+0.001203	test-mlogloss:2.405036+0.001518
[31]	train-mlogloss:2.348200+0.001087	test-mlogloss:2.403556+0.001583
[32]	train-mlogloss:2.344903+0.000839	test-mlogloss:2.401724+0.002016
[33]	train-mlogloss:2.341986+0.000650	test-mlogloss:2.400244+0.002325
[34]	train-mlogloss:2.339237+0.000716	test-mlogloss:2.398939+0.002439
[35]	train-mlogloss:2.336377+0.000875	test-mlogloss:2.397510+0.002218
[36]	train-mlogloss:2.333721+0.000861	test-mlogloss:2.396287+0.002280
[37]	train-mlogloss:2.331093+0.000882	test-mlogloss:2.395055+0.002290
[38]	train-mlogloss:2.328488+0.000929	test-mlogloss:2.393876+0.002049
[39]	train-mlogloss:2.326115+0.000903	test-mlogloss:2.392873+0.002119
[40]	train-mlogloss:2.323878+0.001063	test-mlogloss:2.391958+0.002149
[41]	train-mlogloss:2.321623+0.001115	test-mlogloss:2.390949+0.002207
[42]	train-mlogloss:2.319431+0.000995	test-mlogloss:2.389987+0.002350
[43]	train-mlogloss:2.317335+0.001138	test-mlogloss:2.389204+0.002249
[44]	train-mlogloss:2.315284+0.001143	test-mlogloss:2.388435+0.002114
[45]	train-mlogloss:2.313181+0.000963	test-mlogloss:2.387648+0.002275
[46]	train-mlogloss:2.310978+0.001100	test-mlogloss:2.386632+0.002085
[47]	train-mlogloss:2.308745+0.001153	test-mlogloss:2.385802+0.002047
[48]	train-mlogloss:2.306845+0.001288	test-mlogloss:2.385177+0.001934
[49]	train-mlogloss:2.304977+0.001502	test-mlogloss:2.384506+0.001819
[50]	train-mlogloss:2.302854+0.001370	test-mlogloss:2.383600+0.002013
[51]	train-mlogloss:2.300793+0.001413	test-mlogloss:2.382858+0.002025
[52]	train-mlogloss:2.298853+0.001408	test-mlogloss:2.382212+0.001931
[53]	train-mlogloss:2.296932+0.001442	test-mlogloss:2.381470+0.001873
[54]	train-mlogloss:2.294935+0.001521	test-mlogloss:2.380681+0.001888
[55]	train-mlogloss:2.293066+0.001597	test-mlogloss:2.380054+0.001726
[56]	train-mlogloss:2.291197+0.001665	test-mlogloss:2.379421+0.001688
[57]	train-mlogloss:2.289412+0.001526	test-mlogloss:2.378942+0.001747
[58]	train-mlogloss:2.287629+0.001538	test-mlogloss:2.378331+0.001693
[59]	train-mlogloss:2.285794+0.001459	test-mlogloss:2.377688+0.001790
[60]	train-mlogloss:2.283960+0.001482	test-mlogloss:2.377136+0.001862
[61]	train-mlogloss:2.282280+0.001421	test-mlogloss:2.376652+0.001910
[62]	train-mlogloss:2.280600+0.001233	test-mlogloss:2.376156+0.001977
[63]	train-mlogloss:2.278685+0.001225	test-mlogloss:2.375514+0.001989
[64]	train-mlogloss:2.276869+0.001153	test-mlogloss:2.374929+0.001987
[65]	train-mlogloss:2.275145+0.001197	test-mlogloss:2.374410+0.001912
[66]	train-mlogloss:2.273427+0.001341	test-mlogloss:2.373943+0.001805
[67]	train-mlogloss:2.271796+0.001405	test-mlogloss:2.373440+0.001725
[68]	train-mlogloss:2.270028+0.001195	test-mlogloss:2.372979+0.001889
[69]	train-mlogloss:2.268457+0.001228	test-mlogloss:2.372498+0.001893
[70]	train-mlogloss:2.266904+0.001300	test-mlogloss:2.372106+0.001813
[71]	train-mlogloss:2.265431+0.001184	test-mlogloss:2.371721+0.001862
[72]	train-mlogloss:2.263897+0.001114	test-mlogloss:2.371297+0.001868
[73]	train-mlogloss:2.262246+0.001073	test-mlogloss:2.370789+0.001868
[74]	train-mlogloss:2.260613+0.001214	test-mlogloss:2.370354+0.001836
[75]	train-mlogloss:2.259096+0.001420	test-mlogloss:2.369943+0.001669
[76]	train-mlogloss:2.257564+0.001511	test-mlogloss:2.369524+0.001571
[77]	train-mlogloss:2.255876+0.001588	test-mlogloss:2.369047+0.001480
[78]	train-mlogloss:2.254201+0.001365	test-mlogloss:2.368578+0.001659
[79]	train-mlogloss:2.252605+0.001276	test-mlogloss:2.368180+0.001736
[80]	train-mlogloss:2.250962+0.001361	test-mlogloss:2.367633+0.001665
[81]	train-mlogloss:2.249665+0.001427	test-mlogloss:2.367384+0.001591
[82]	train-mlogloss:2.248198+0.001415	test-mlogloss:2.367033+0.001634
[83]	train-mlogloss:2.246647+0.001550	test-mlogloss:2.366596+0.001572
[84]	train-mlogloss:2.245100+0.001675	test-mlogloss:2.366173+0.001484
[85]	train-mlogloss:2.243729+0.001604	test-mlogloss:2.365886+0.001549
[86]	train-mlogloss:2.242267+0.001420	test-mlogloss:2.365541+0.001652
[87]	train-mlogloss:2.240895+0.001463	test-mlogloss:2.365256+0.001684
[88]	train-mlogloss:2.239403+0.001497	test-mlogloss:2.364912+0.001609
[89]	train-mlogloss:2.238047+0.001334	test-mlogloss:2.364629+0.001616
[90]	train-mlogloss:2.236700+0.001274	test-mlogloss:2.364375+0.001646
[91]	train-mlogloss:2.235415+0.001357	test-mlogloss:2.364124+0.001588
[92]	train-mlogloss:2.233976+0.001394	test-mlogloss:2.363791+0.001535
[93]	train-mlogloss:2.232703+0.001345	test-mlogloss:2.363551+0.001594
[94]	train-mlogloss:2.231267+0.001218	test-mlogloss:2.363184+0.001660
[95]	train-mlogloss:2.229960+0.001268	test-mlogloss:2.362895+0.001544
[96]	train-mlogloss:2.228653+0.001363	test-mlogloss:2.362659+0.001561
[97]	train-mlogloss:2.227406+0.001397	test-mlogloss:2.362423+0.001518
[98]	train-mlogloss:2.225996+0.001414	test-mlogloss:2.362071+0.001454
[99]	train-mlogloss:2.224531+0.001378	test-mlogloss:2.361742+0.001369
[100]	train-mlogloss:2.223227+0.001281	test-mlogloss:2.361473+0.001480
[101]	train-mlogloss:2.221762+0.001353	test-mlogloss:2.361129+0.001390
[102]	train-mlogloss:2.220428+0.001424	test-mlogloss:2.360816+0.001427
[103]	train-mlogloss:2.219146+0.001387	test-mlogloss:2.360564+0.001486
[104]	train-mlogloss:2.217850+0.001385	test-mlogloss:2.360342+0.001483
[105]	train-mlogloss:2.216579+0.001346	test-mlogloss:2.360078+0.001513
[106]	train-mlogloss:2.215298+0.001350	test-mlogloss:2.359864+0.001530
[107]	train-mlogloss:2.213951+0.001492	test-mlogloss:2.359598+0.001415
[108]	train-mlogloss:2.212674+0.001496	test-mlogloss:2.359418+0.001425
[109]	train-mlogloss:2.211389+0.001386	test-mlogloss:2.359189+0.001449
[110]	train-mlogloss:2.210156+0.001368	test-mlogloss:2.358995+0.001472
[111]	train-mlogloss:2.208966+0.001483	test-mlogloss:2.358786+0.001434
[112]	train-mlogloss:2.207654+0.001516	test-mlogloss:2.358484+0.001495
[113]	train-mlogloss:2.206629+0.001672	test-mlogloss:2.358342+0.001451
[114]	train-mlogloss:2.205502+0.001752	test-mlogloss:2.358183+0.001430
[115]	train-mlogloss:2.204295+0.001659	test-mlogloss:2.357941+0.001449
[116]	train-mlogloss:2.203037+0.001649	test-mlogloss:2.357711+0.001456
[117]	train-mlogloss:2.201810+0.001766	test-mlogloss:2.357472+0.001418
[118]	train-mlogloss:2.200571+0.001776	test-mlogloss:2.357266+0.001414
[119]	train-mlogloss:2.199362+0.001800	test-mlogloss:2.357079+0.001358
[120]	train-mlogloss:2.198256+0.001794	test-mlogloss:2.356926+0.001332
[121]	train-mlogloss:2.196956+0.001787	test-mlogloss:2.356691+0.001352
[122]	train-mlogloss:2.195768+0.001822	test-mlogloss:2.356499+0.001333
[123]	train-mlogloss:2.194525+0.001817	test-mlogloss:2.356259+0.001354
[124]	train-mlogloss:2.193267+0.001783	test-mlogloss:2.356056+0.001398
[125]	train-mlogloss:2.192097+0.001700	test-mlogloss:2.355873+0.001427
[126]	train-mlogloss:2.191004+0.001560	test-mlogloss:2.355701+0.001486
[127]	train-mlogloss:2.189887+0.001554	test-mlogloss:2.355521+0.001543
[128]	train-mlogloss:2.188699+0.001649	test-mlogloss:2.355339+0.001542
[129]	train-mlogloss:2.187565+0.001663	test-mlogloss:2.355195+0.001511
[130]	train-mlogloss:2.186398+0.001624	test-mlogloss:2.355024+0.001498
[131]	train-mlogloss:2.185206+0.001660	test-mlogloss:2.354832+0.001491
[132]	train-mlogloss:2.184016+0.001683	test-mlogloss:2.354669+0.001473
[133]	train-mlogloss:2.182889+0.001671	test-mlogloss:2.354532+0.001429
[134]	train-mlogloss:2.181750+0.001730	test-mlogloss:2.354371+0.001455
[135]	train-mlogloss:2.180576+0.001750	test-mlogloss:2.354189+0.001487
[136]	train-mlogloss:2.179481+0.001580	test-mlogloss:2.354070+0.001558
[137]	train-mlogloss:2.178321+0.001352	test-mlogloss:2.353890+0.001693
[138]	train-mlogloss:2.177228+0.001262	test-mlogloss:2.353785+0.001746
[139]	train-mlogloss:2.176129+0.001297	test-mlogloss:2.353644+0.001706
[140]	train-mlogloss:2.175110+0.001165	test-mlogloss:2.353527+0.001746
[141]	train-mlogloss:2.173922+0.001021	test-mlogloss:2.353344+0.001798
[142]	train-mlogloss:2.172769+0.000985	test-mlogloss:2.353164+0.001849
[143]	train-mlogloss:2.171630+0.000996	test-mlogloss:2.352998+0.001885
[144]	train-mlogloss:2.170532+0.000979	test-mlogloss:2.352859+0.001977
[145]	train-mlogloss:2.169341+0.000994	test-mlogloss:2.352697+0.001946
[146]	train-mlogloss:2.168111+0.001001	test-mlogloss:2.352487+0.001918
[147]	train-mlogloss:2.167048+0.001040	test-mlogloss:2.352365+0.001929
[148]	train-mlogloss:2.165866+0.001068	test-mlogloss:2.352198+0.002007
[149]	train-mlogloss:2.164826+0.001035	test-mlogloss:2.352127+0.002027
[150]	train-mlogloss:2.163746+0.001009	test-mlogloss:2.352000+0.002065
[151]	train-mlogloss:2.162621+0.001016	test-mlogloss:2.351859+0.002070
[152]	train-mlogloss:2.161532+0.000999	test-mlogloss:2.351723+0.002073
[153]	train-mlogloss:2.160539+0.000938	test-mlogloss:2.351594+0.002100
[154]	train-mlogloss:2.159585+0.001008	test-mlogloss:2.351509+0.002137
[155]	train-mlogloss:2.158624+0.001115	test-mlogloss:2.351460+0.002134
[156]	train-mlogloss:2.157665+0.001177	test-mlogloss:2.351396+0.002122
[157]	train-mlogloss:2.156583+0.001158	test-mlogloss:2.351264+0.002118
[158]	train-mlogloss:2.155569+0.001137	test-mlogloss:2.351176+0.002089
[159]	train-mlogloss:2.154587+0.001330	test-mlogloss:2.351055+0.002030
[160]	train-mlogloss:2.153558+0.001384	test-mlogloss:2.350935+0.002046
[161]	train-mlogloss:2.152488+0.001362	test-mlogloss:2.350827+0.002068
[162]	train-mlogloss:2.151491+0.001435	test-mlogloss:2.350752+0.002083
[163]	train-mlogloss:2.150467+0.001455	test-mlogloss:2.350636+0.002046
[164]	train-mlogloss:2.149536+0.001565	test-mlogloss:2.350571+0.002004
[165]	train-mlogloss:2.148484+0.001560	test-mlogloss:2.350467+0.001993
[166]	train-mlogloss:2.147448+0.001504	test-mlogloss:2.350328+0.001980
[167]	train-mlogloss:2.146494+0.001480	test-mlogloss:2.350238+0.001894
[168]	train-mlogloss:2.145466+0.001605	test-mlogloss:2.350130+0.001844
[169]	train-mlogloss:2.144560+0.001676	test-mlogloss:2.350066+0.001834
[170]	train-mlogloss:2.143627+0.001635	test-mlogloss:2.349986+0.001848
[171]	train-mlogloss:2.142639+0.001646	test-mlogloss:2.349891+0.001845
[172]	train-mlogloss:2.141619+0.001614	test-mlogloss:2.349784+0.001765
[173]	train-mlogloss:2.140607+0.001687	test-mlogloss:2.349668+0.001697
[174]	train-mlogloss:2.139642+0.001822	test-mlogloss:2.349602+0.001650
[175]	train-mlogloss:2.138671+0.001841	test-mlogloss:2.349534+0.001668
[176]	train-mlogloss:2.137661+0.001782	test-mlogloss:2.349433+0.001729
[177]	train-mlogloss:2.136553+0.001741	test-mlogloss:2.349333+0.001696
[178]	train-mlogloss:2.135532+0.001695	test-mlogloss:2.349217+0.001724
[179]	train-mlogloss:2.134444+0.001679	test-mlogloss:2.349089+0.001698
[180]	train-mlogloss:2.133478+0.001614	test-mlogloss:2.349012+0.001727
[181]	train-mlogloss:2.132445+0.001536	test-mlogloss:2.348910+0.001755
[182]	train-mlogloss:2.131447+0.001468	test-mlogloss:2.348810+0.001739
[183]	train-mlogloss:2.130499+0.001352	test-mlogloss:2.348737+0.001819
[184]	train-mlogloss:2.129548+0.001357	test-mlogloss:2.348663+0.001883
[185]	train-mlogloss:2.128589+0.001292	test-mlogloss:2.348552+0.001881
[186]	train-mlogloss:2.127581+0.001253	test-mlogloss:2.348438+0.001845
[187]	train-mlogloss:2.126656+0.001346	test-mlogloss:2.348382+0.001819
[188]	train-mlogloss:2.125674+0.001363	test-mlogloss:2.348299+0.001804
[189]	train-mlogloss:2.124697+0.001347	test-mlogloss:2.348188+0.001804
[190]	train-mlogloss:2.123693+0.001325	test-mlogloss:2.348136+0.001814
[191]	train-mlogloss:2.122811+0.001289	test-mlogloss:2.348110+0.001833
[192]	train-mlogloss:2.121862+0.001418	test-mlogloss:2.348018+0.001817
[193]	train-mlogloss:2.120933+0.001419	test-mlogloss:2.347946+0.001869
[194]	train-mlogloss:2.120081+0.001433	test-mlogloss:2.347892+0.001827
[195]	train-mlogloss:2.119138+0.001431	test-mlogloss:2.347816+0.001780
[196]	train-mlogloss:2.118253+0.001474	test-mlogloss:2.347741+0.001776
[197]	train-mlogloss:2.117334+0.001428	test-mlogloss:2.347691+0.001807
[198]	train-mlogloss:2.116408+0.001385	test-mlogloss:2.347645+0.001826
[199]	train-mlogloss:2.115474+0.001432	test-mlogloss:2.347581+0.001816
> bst.cv
     train.mlogloss.mean train.mlogloss.std test.mlogloss.mean
  1:            3.131484           0.002547           3.135426
  2:            2.945235           0.000145           2.951257
  3:            2.825576           0.000966           2.833698
  4:            2.741026           0.000662           2.750912
  5:            2.677405           0.000747           2.688952
 ---                                                          
196:            2.119138           0.001431           2.347816
197:            2.118253           0.001474           2.347741
198:            2.117334           0.001428           2.347691
199:            2.116408           0.001385           2.347645
200:            2.115474           0.001432           2.347581
     test.mlogloss.std
  1:          0.003619
  2:          0.001682
  3:          0.002453
  4:          0.002179
  5:          0.002348
 ---                  
196:          0.001780
197:          0.001776
198:          0.001807
199:          0.001826
200:          0.001816
> 
> print('training with xgboost')
[1] "training with xgboost"
> nround = 50
> bst = xgboost(param=param, data = trainMatrix, label = y, nrounds=nround)
[0]	train-mlogloss:3.133246
[1]	train-mlogloss:2.948151
[2]	train-mlogloss:2.828088
[3]	train-mlogloss:2.743820
[4]	train-mlogloss:2.679986
[5]	train-mlogloss:2.630523
[6]	train-mlogloss:2.591258
[7]	train-mlogloss:2.559014
[8]	train-mlogloss:2.532239
[9]	train-mlogloss:2.510940
[10]	train-mlogloss:2.491895
[11]	train-mlogloss:2.476361
[12]	train-mlogloss:2.463007
[13]	train-mlogloss:2.450708
[14]	train-mlogloss:2.441093
[15]	train-mlogloss:2.432577
[16]	train-mlogloss:2.424969
[17]	train-mlogloss:2.418327
[18]	train-mlogloss:2.411948
[19]	train-mlogloss:2.405570
[20]	train-mlogloss:2.400551
[21]	train-mlogloss:2.395716
[22]	train-mlogloss:2.391862
[23]	train-mlogloss:2.387533
[24]	train-mlogloss:2.383219
[25]	train-mlogloss:2.379521
[26]	train-mlogloss:2.375575
[27]	train-mlogloss:2.372142
[28]	train-mlogloss:2.368834
[29]	train-mlogloss:2.366014
[30]	train-mlogloss:2.363500
[31]	train-mlogloss:2.360507
[32]	train-mlogloss:2.357747
[33]	train-mlogloss:2.355457
[34]	train-mlogloss:2.352496
[35]	train-mlogloss:2.350491
[36]	train-mlogloss:2.348154
[37]	train-mlogloss:2.345773
[38]	train-mlogloss:2.343559
[39]	train-mlogloss:2.341701
[40]	train-mlogloss:2.339611
[41]	train-mlogloss:2.337241
[42]	train-mlogloss:2.335105
[43]	train-mlogloss:2.333292
[44]	train-mlogloss:2.331431
[45]	train-mlogloss:2.329514
[46]	train-mlogloss:2.327575
[47]	train-mlogloss:2.325458
[48]	train-mlogloss:2.323922
[49]	train-mlogloss:2.322296
> 
> 
> # Model interpretation ----------------------------------------------------
> 
> # Importance matrix
> names <- dimnames(trainMatrix)[[2]]
> importance_matrix <- xgb.importance(names, model = bst)
> 
> # Nice graph
> require(Ckmeans.1d.dp)
Loading required package: Ckmeans.1d.dp
> xgb.plot.importance(importance_matrix)
> 
> # Plot tree
> require(DiagrammeR)
Loading required package: DiagrammeR
> xgb.plot.tree(feature_names = names, model = bst, n_first_tree = 2)
> 
> 
> # Save workspace ----------------------------------------------------------
> 
> save.image("workspace.RData")
> 
> 
> proc.time()
      user     system    elapsed 
114720.984   1748.718  33809.003 
