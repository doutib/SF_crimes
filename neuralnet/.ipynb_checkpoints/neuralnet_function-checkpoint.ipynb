{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_layers_nnet(X,\n",
    "                    Y,\n",
    "                    prop_train=0.5,\n",
    "                    method1=\"Tanh\",\n",
    "                    neurons1=5,\n",
    "                    method2=\"\",\n",
    "                    neurons2=0,\n",
    "                    decay=0.0001,\n",
    "                    learning_rate=0.001,\n",
    "                    n_iter=25,\n",
    "                    random_state=1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X             : pandas data frame\n",
    "        data frame of features\n",
    "    Y             : pandas data frame\n",
    "        data frame of labels\n",
    "    prop_train    : float\n",
    "        proportion of the traning set\n",
    "    method1       : str\n",
    "        method used for the first layer\n",
    "    neurons1      : int\n",
    "        number of neurons of the first layer\n",
    "    method2       : None\n",
    "        method used for the first layer\n",
    "    neurons2      : int\n",
    "        number of neurons of the first layer\n",
    "    decay         : float\n",
    "        weight decay\n",
    "    learning_rate : float\n",
    "        learning rate\n",
    "    n_iter        : int\n",
    "        number of iterations\n",
    "    random_state  : int\n",
    "        seed for weight initialization\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    numpy array\n",
    "        logloss    : averaged logarithmic loss\n",
    "        miss_err   : missclassification error rate\n",
    "        prec       : precision\n",
    "        recall     : recall\n",
    "        f1         : f1 score\n",
    "        parameters : previous parameters in the order previously specified\n",
    "    \"\"\"\n",
    "\n",
    "    labels = np.unique(Y)\n",
    "    \n",
    "    ## # Scale Data\n",
    "    scaler = MinMaxScaler()\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)\n",
    "\n",
    "    ## # Split data set into train/test\n",
    "    np.random.seed(seed=1)\n",
    "    msk = np.random.rand(len(X)) < prop_train\n",
    "    X_train = np.array(X[msk])\n",
    "    Y_train = np.array(Y[msk])\n",
    "    X_test =  np.array(X[~msk])\n",
    "    Y_test =  np.array(Y[~msk])\n",
    "    \n",
    "    # Layers\n",
    "    if neurons2 == 0 :\n",
    "        layers=[Layer(method1, weight_decay = decay, units = neurons1),\n",
    "                Layer(\"Softmax\")]\n",
    "    else:\n",
    "        layers=[Layer(method1, weight_decay = decay, units = neurons1),\n",
    "                Layer(method2, weight_decay = decay, units = neurons2),\n",
    "                Layer(\"Softmax\")]\n",
    "        \n",
    "    ## # Run nnet\n",
    "    # Define classifier\n",
    "    nn = Classifier(layers,\n",
    "                    learning_rate=learning_rate,\n",
    "                    random_state=random_state,\n",
    "                    n_iter=n_iter)\n",
    "    # Fit\n",
    "    nn.fit(X_train, Y_train)\n",
    "    # Predict\n",
    "    Y_hat = nn.predict(X_test)\n",
    "    Y_probs = nn.predict_proba(X_test)\n",
    "    \n",
    "    ## # Misclassification error rate\n",
    "    miss_err = float(sum(Y_test[:,0]!=Y_hat[:,0]))/float(len(Y_test[:,0]))\n",
    "    ## # Log Loss\n",
    "    eps = 10^(-15)\n",
    "    logloss = log_loss(Y_test, Y_probs, eps = eps)\n",
    "    \n",
    "    ## # Precision\n",
    "    prec = precision_score(y_true=Y_test, y_pred=Y_hat, labels=labels, average='micro')\n",
    "    ## # Recal\n",
    "    recall = recall_score(y_true=Y_test, y_pred=Y_hat, labels=labels, average='micro') \n",
    "    ## # F1\n",
    "    f1 = f1_score(y_true=Y_test, y_pred=Y_hat, labels=labels, average='micro')\n",
    "    \n",
    "    # Summarized results\n",
    "    result = np.array([logloss,\n",
    "                       miss_err,\n",
    "                       prec,\n",
    "                       recall,\n",
    "                       f1,\n",
    "                       method1,\n",
    "                       neurons1,\n",
    "                       method2,\n",
    "                       neurons2,\n",
    "                       decay,\n",
    "                       learning_rate,\n",
    "                       n_iter,\n",
    "                       random_state,\n",
    "                       prop_train])\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
